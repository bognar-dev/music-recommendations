# -*- coding: utf-8 -*-
"""voyager.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LcY-HcksJi79uoRlRfuDctAqdsDRfv_h
"""

# pip install pandas numpy scikit-learn voyager kneed

"""Connect to Google Drive"""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Set the path to your data folder
data_folder = '/content/drive/MyDrive/complete_data_set_27_02_25'

# Verify the folder exists
if os.path.exists(data_folder):
    print(f"Successfully connected to: {data_folder}")
    # List contents to verify
    print("\nContents:")
    for item in os.listdir(data_folder)[:10]:  # Show first 10 items
        print(f"- {item}")
    if len(os.listdir(data_folder)) > 10:
        print(f"... and {len(os.listdir(data_folder)) - 10} more items")
else:
    print(f"Error: Folder not found at {data_folder}")
    # Try to find the folder by searching common locations
    possible_locations = [
        '/content/drive/MyDrive',
        '/content/drive/Shared drives'
    ]

    found = False
    for location in possible_locations:
        if os.path.exists(location):
            for root, dirs, _ in os.walk(location):
                if "complete_data_set_27_02_25" in dirs:
                    found_path = os.path.join(root, "complete_data_set_27_02_25")
                    print(f"Found data folder at: {found_path}")
                    print("Please update your data_folder variable to this path")
                    found = True
                    break
            if found:
                break

"""1. Setup and data loading"""

# Imports
import pandas as pd
import numpy as np
import pickle
from sklearn.preprocessing import StandardScaler
import os
from voyager import Index, Space, StorageDataType

# Load the CSV data into a pandas DataFrame
print("Loading data...")
df_uncleaned = pd.read_csv(os.path.join(data_folder, "cleaned_data_both_efficientnet_v2_features.csv"))
print(f"Original DataFrame loaded successfully. Shape: {df_uncleaned.shape}")


# Filter out rows where 'preview_url' or 'image_url' are NaN, 'no', or empty string
df = df_uncleaned[~df_uncleaned['preview'].isin([None, 'no', '']) & ~df_uncleaned['image_path'].isin([None, 'no', ''])]
df.reset_index(drop=True, inplace=True)
print(f"Cleaned DataFrame size: {df.shape}")
df.head()

"""Feature Visualization

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.gridspec import GridSpec
import os
import json

def calculate_statistics(df):
    """
    Calculates statistics for each column of a pandas DataFrame.

    Args:
        df (pd.DataFrame): The input DataFrame.

    Returns:
        dict: A dictionary where keys are column names and values
              are dictionaries containing the calculated statistics.
    """
    stats = {}
    for col in df.columns:
        col_stats = {}
        series = df[col]

        # Basic statistics applicable to all columns
        col_stats["Valid"] = int(series.count())
        col_stats["Missing"] = int(series.isnull().sum())
        col_stats["Mismatched"] = 0  # Initialize, may be updated later

        # Try converting to numeric if its object, numeric already numeric series remain as numeric

        try:
          numeric_series = pd.to_numeric(series, errors='raise')
          is_numeric = True
        except:
          is_numeric = False

        if is_numeric:
            # Statistics applicable if succesfully converted
            try:
                col_stats["Unique"] = int(numeric_series.nunique())
                col_stats["Mean"] = float(numeric_series.mean()) if not pd.isna(numeric_series.mean()) else None
                col_stats["Std. Deviation"] = float(numeric_series.std()) if not pd.isna(numeric_series.std()) else None
                col_stats["Min"] = float(numeric_series.min()) if not pd.isna(numeric_series.min()) else None
                col_stats["25%"] = float(numeric_series.quantile(0.25)) if not pd.isna(numeric_series.quantile(0.25)) else None
                col_stats["50%"] = float(numeric_series.quantile(0.50)) if not pd.isna(numeric_series.quantile(0.50)) else None
                col_stats["75%"] = float(numeric_series.quantile(0.75)) if not pd.isna(numeric_series.quantile(0.75)) else None
                col_stats["Max"] = float(numeric_series.max()) if not pd.isna(numeric_series.max()) else None

                try:
                    bins = pd.cut(numeric_series.dropna(), bins=10)
                    bin_counts = bins.value_counts().sort_index()

                    col_stats["Label Distribution"] = {
                        str(interval): int(count)
                        for interval, count in zip(bin_counts.index, bin_counts.values)
                    }
                except Exception as e:
                    col_stats["Label Distribution"] = f"Binning failed: {str(e)}"
            except Exception as generalFailure:
                print(f"General failure in a numeric section{str(generalFailure)}")

        else:  # Non-numeric columns
            col_stats["Unique"] = int(series.nunique())
            most_common = series.mode()
            if not most_common.empty:  # Handle cases where mode is empty
                col_stats["Most Common"] = str(most_common.iloc[0])  # Take the first if multiple
            else:
                col_stats["Most Common"] = None

            # Example for string columns to identify values from non values
            if pd.api.types.is_string_dtype(series) or series.dtype == 'object':
                value_counts = series.value_counts(dropna=False)
                if not value_counts.empty:
                    # Convert to a more readable format
                    if len(value_counts) > 10:
                        # Show top 3 and group the rest
                        top_values = value_counts.head(3)
                        other_count = value_counts[3:].sum()

                        col_stats["Value Counts"] = {
                            str(val): int(count) for val, count in top_values.items()
                        }
                        col_stats["Value Counts"][f"Other ({len(value_counts) - 3})"] = int(other_count)
                    else:
                        col_stats["Value Counts"] = {
                            str(val): int(count) for val, count in value_counts.items()
                        }

        stats[col] = col_stats
    return stats


def create_column_visualization(df, column_name, stats, output_dir='column_visualizations'):
    """
    Creates a visualization for a specific column based on its data type.

    Args:
        df: pandas DataFrame
        column_name: name of the column to visualize
        stats: statistics dictionary for this column
        output_dir: directory to save the visualizations
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Get the series
    series = df[column_name]

    # Get statistics from the stats dictionary
    valid_count = stats["Valid"]
    total_count = len(series)
    valid_percent = (valid_count / total_count) * 100 if total_count > 0 else 0
    missing_count = stats["Missing"]
    missing_percent = (missing_count / total_count) * 100 if total_count > 0 else 0
    mismatched_count = stats["Mismatched"]
    mismatched_percent = (mismatched_count / total_count) * 100 if total_count > 0 else 0

    # Create figure
    fig = plt.figure(figsize=(10, 6))
    gs = GridSpec(1, 2, width_ratios=[1, 1])

    # Try converting to numeric before visualisation (this handles mixed objects for the plot to show as numerical.)
    try:
          numeric_series = pd.to_numeric(series, errors='raise')
          is_numeric = True
    except:
          is_numeric = False
    # Determine if column is numeric
    #is_numeric = pd.api.types.is_numeric_dtype(series) and not pd.api.types.is_bool_dtype(series)

    if is_numeric:
        # For numeric columns
        ax1 = fig.add_subplot(gs[0])

        # Create histogram
        if not series.dropna().empty:
            sns.histplot(numeric_series.dropna(), ax=ax1, color='#0072B2')

            # Add min and max labels
            min_val = stats.get("Min", "N/A")
            max_val = stats.get("Max", "N/A")
            if min_val is not None and min_val!= "N/A":
                ax1.text(min_val, 0, f"{min_val:.2f}" if isinstance(min_val, float) else str(min_val),
                        ha='left', va='bottom')
            if max_val is not None and max_val != "N/A":
                ax1.text(max_val, 0, f"{max_val:.2f}" if isinstance(max_val, float) else str(max_val),
                        ha='right', va='bottom')
        else:
            ax1.text(0.5, 0.5, "No data to display", ha='center', va='center', transform=ax1.transAxes)
            ax1.set_xticks([])
            ax1.set_yticks([])

        # Stats panel
        ax2 = fig.add_subplot(gs[1])
        ax2.axis('off')

        # Get statistics
        mean = stats.get("Mean", "N/A")
        std = stats.get("Std. Deviation", "N/A")
        min_val = stats.get("Min", "N/A")
        q25 = stats.get("25%", "N/A")
        q50 = stats.get("50%", "N/A")
        q75 = stats.get("75%", "N/A")
        max_val = stats.get("Max", "N/A")


        # Format numeric values
        mean_str = f"{mean:.2f}" if isinstance(mean, (int, float)) else str(mean)
        std_str = f"{std:.2f}" if isinstance(std, (int, float)) else str(std)
        min_str = f"{min_val:.2f}" if isinstance(min_val, (int, float)) else str(min_val)
        q25_str = f"{q25:.2f}" if isinstance(q25, (int, float)) else str(q25)
        q50_str = f"{q50:.2f}" if isinstance(q50, (int, float)) else str(q50)
        q75_str = f"{q75:.2f}" if isinstance(q75, (int, float)) else str(q75)
        max_str = f"{max_val:.2f}" if isinstance(max_val, (int, float)) else str(max_val)

        # Create stats table
        stats_text = [
            f"Valid █", f"{valid_count:,}", f"{valid_percent:.0f}%",
            f"Mismatched █", f"{mismatched_count:,}", f"{mismatched_percent:.0f}%",
            f"Missing █", f"{missing_count:,}", f"{missing_percent:.0f}%",
            "",
            f"Mean", mean_str,
            f"Std. Deviation", std_str,
            "",
            f"Quantiles",
            min_str, f"Min",
            q25_str, f"25%",
            q50_str, f"50%",
            q75_str, f"75%",
            max_str, f"Max"
        ]

        # Position the text
        y_pos = 0.95
        for i in range(0, len(stats_text), 3):
            if i+2 < len(stats_text):
                ax2.text(0.1, y_pos, stats_text[i], ha='left', fontweight='bold')
                ax2.text(0.7, y_pos, stats_text[i+1], ha='right')
                ax2.text(0.95, y_pos, stats_text[i+2], ha='right', color='#666666')
            elif i+1 < len(stats_text):
                ax2.text(0.1, y_pos, stats_text[i], ha='left', fontweight='bold')
                ax2.text(0.7, y_pos, stats_text[i+1], ha='right')
            else:
                ax2.text(0.1, y_pos, stats_text[i], ha='left', fontweight='bold')
            y_pos -= 0.05

        # Add a green bar at the top
        ax2.axhspan(0.97, 1.0, facecolor='green', alpha=0.3)

    else:
        # For string/categorical columns
        ax1 = fig.add_subplot(gs[0])

        # Get value counts if available
        if "Value Counts" in stats:
            value_counts = pd.Series(stats["Value Counts"])

            # Plot as text
            ax1.axis('off')
            y_pos = 0.9
            for value, count in value_counts.items():
                percentage = (count / total_count) * 100 if total_count > 0 else 0
                ax1.text(0.1, y_pos, str(value)[:100], ha='left') # Truncate string values
                ax1.text(0.9, y_pos, f"{percentage:.0f}%", ha='right')
                y_pos -= 0.2
        else:
            # If no value counts available
            ax1.axis('off')
            ax1.text(0.5, 0.5, "No categorical data to display", ha='center', va='center', transform=ax1.transAxes)

        # Stats panel
        ax2 = fig.add_subplot(gs[1])
        ax2.axis('off')

        # Get unique count and most common
        unique_count = stats.get("Unique", "N/A")
        most_common = stats.get("Most Common", "None")

        # Calculate percentage of most common value
        most_common_percent = 0
        if "Value Counts" in stats and most_common in stats["Value Counts"]:
            most_common_count = stats["Value Counts"][most_common]
            most_common_percent = (most_common_count / total_count) * 100 if total_count > 0 else 0

        # Create stats table
        stats_text = [
            f"Valid █", f"{valid_count:,}", f"{valid_percent:.0f}%",
            f"Mismatched █", f"{mismatched_count:,}", f"{mismatched_percent:.0f}%",
            f"Missing █", f"{missing_count:,}", f"{missing_percent:.0f}%",
            "",
            f"Unique", f"{unique_count:,}",
            f"Most Common", str(most_common)[:100], f"{most_common_percent:.0f}%" # Truncate most common as well
        ]

        # Position the text
        y_pos = 0.95
        for i in range(0, len(stats_text), 3):
            if i+2 < len(stats_text):
                ax2.text(0.1, y_pos, stats_text[i], ha='left', fontweight='bold')
                ax2.text(0.7, y_pos, stats_text[i+1], ha='right')
                ax2.text(0.95, y_pos, stats_text[i+2], ha='right', color='#666666')
            elif i+1 < len(stats_text):
                ax2.text(0.1, y_pos, stats_text[i], ha='left', fontweight='bold')
                ax2.text(0.7, y_pos, stats_text[i+1], ha='right')
            else:
                ax2.text(0.1, y_pos, stats_text[i], ha='left', fontweight='bold')
            y_pos -= 0.05

        # Add a green bar at the top
        ax2.axhspan(0.97, 1.0, facecolor='green', alpha=0.3)

    # Add title
    column_name_truncated = column_name[:50] # Truncated to avoid long titles
    if is_numeric:
        plt.suptitle(f"# {column_name_truncated}", fontsize=14, fontweight='bold', ha='left', x=0.1)
    else:
        plt.suptitle(f"△ {column_name_truncated}", fontsize=14, fontweight='bold', ha='left', x=0.1)

    # Save the figure
    plt.tight_layout()
    plt.savefig(f"{output_dir}/{column_name[:50]}_visualization.png", dpi=100, bbox_inches='tight')
    plt.close()


def process_csv_file(csv_file, data_folder, output_dir='column_visualizations', save_stats=True, sep=','):
    """
    Processes a CSV file: calculates statistics and creates visualizations.

    Args:
        csv_file: name of the CSV file
        data_folder: path to the data directory
        output_dir: directory to save the visualizations
        save_stats: whether to save statistics to a JSON file
        sep: The separator used in the CSV file. Defaults to comma (',').
    """
    try:
        # Create full file path using os.path.join
        csv_path = os.path.join(data_folder, csv_file)

        # Load the CSV file with specified separator
        df = pd.read_csv(csv_path, sep=sep, low_memory=False)
        print(f"Successfully loaded CSV file: {csv_path}")
        print(f"Shape: {df.shape} (rows, columns)")

        # Calculate statistics
        print("Calculating statistics...")
        column_stats = calculate_statistics(df)

        # Save statistics to JSON if requested
        if save_stats:
            stats_file = f"{output_dir}/column_statistics.json"
            os.makedirs(output_dir, exist_ok=True)
            with open(stats_file, 'w') as f:
                json.dump(column_stats, f, indent=4)
            print(f"Statistics saved to {stats_file}")

        # Print statistics to console
        for col, stats in column_stats.items():
            print(f"Column: {col}")
            for stat, value in stats.items():
                print(f"  {stat}: {value}")
            print("-" * 30)

        # Create visualizations
        print("Creating visualizations...")
        for column in df.columns:
            print(f"Processing column: {column}")
            create_column_visualization(df, column, column_stats[column], output_dir)

        print(f"Visualizations saved to {output_dir}/")

    except FileNotFoundError:
        print(f"Error: File not found: {csv_path}")  # Use constructed path for error message
        exit()
    except Exception as e:
        print(f"Error processing CSV: {e}")
        import traceback
        traceback.print_exc()
        exit()


if __name__ == "__main__":
    # Specify the data folder
    data_folder = "/content/drive/MyDrive/complete_data_set_27_02_25"

    # Replace with your CSV file name
    csv_file = "cleaned_data_both_efficientnet_v2_features.csv"

    process_csv_file(csv_file, data_folder, sep=',')  # Assuming comma is the default separator

"""2. Feature Loading and Filtering"""

# Load the deep learning features from pickle file
print("Loading deep learning features from pickle file...")
pickle_path = os.path.join(data_folder,"cleaned_data_both_efficientnet_v2_features.pkl")  # Update this path to your pickle file

if os.path.exists(pickle_path):
    with open(pickle_path, 'rb') as f:
        features_dict = pickle.load(f)
    print(f"Loaded features for {len(features_dict)} songs")
else:
    print(f"Warning: Pickle file {pickle_path} not found. Proceeding without deep learning features.")
    features_dict = {}

# Create a mapping from DataFrame index to original index
df_index_to_original = {i: idx for i, idx in enumerate(df.index)}
original_to_df_index = {idx: i for i, idx in df_index_to_original.items()}

# Determine the expected feature shape BEFORE filtering
if features_dict:
    sample_feature = next(iter(features_dict.values()))
    expected_feature_shape = sample_feature.shape
    print(f"Expected feature shape: {expected_feature_shape}")
else:
    expected_feature_shape = None
    print("No features in features_dict to determine expected shape.")

# Filter to only include songs that have deep learning features
if features_dict:
    # Convert original indices in features_dict to new DataFrame indices
    valid_indices = [original_to_df_index.get(idx) for idx in features_dict.keys()
                     if idx in original_to_df_index]
    valid_indices = [idx for idx in valid_indices if idx is not None]

    # Filter DataFrame to only include rows with features
    df = df.iloc[valid_indices].reset_index(drop=True)
    print(f"DataFrame filtered to songs with deep features. New shape: {df.shape}")

    # Create a new mapping after filtering
    df_index_to_original = {i: idx for i, idx in enumerate(df.index)}
    original_to_df_index = {idx: i for i, idx in df_index_to_original.items()}

"""3. Feature Extraction"""

# Extract deep learning features in the same order as DataFrame
if features_dict:
    deep_features = []
    for i in range(len(df)):
        original_idx = df_index_to_original[i]
        if original_idx in features_dict:
            feature = features_dict[original_idx]
            # Check if the feature shape matches the expected shape
            if feature.shape == expected_feature_shape:
                deep_features.append(feature)
            else:
                print(f"Warning: Feature shape mismatch for index {i} (original index {original_idx}).")
                print(f"  Expected shape: {expected_feature_shape}, Actual shape: {feature.shape}")
                print("  Using zeros as placeholder.")
                deep_features.append(np.zeros(expected_feature_shape))
        else:
            print(f"Warning: Missing features for index {i} (original index {original_idx})")
            print("  Using zeros as placeholder.")
            deep_features.append(np.zeros(expected_feature_shape))

    deep_features = np.array(deep_features)
    print(f"Deep learning features shape: {deep_features.shape}")

    # Check for NaN or inf in deep_features
    if np.isnan(deep_features).any() or np.isinf(deep_features).any():
        print("Warning: NaN or inf values found in deep_features. Replacing with zeros.")
        deep_features = np.nan_to_num(deep_features)  # Replace NaN/inf with 0

else:
    deep_features = None
    print("No deep learning features available.")

# Extract non-image features (e.g., danceability, energy, etc.)
audio_features = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness',
                 'instrumentalness', 'liveness', 'valence', 'tempo']
non_image_features = df[audio_features].values

# Check for NaN or inf in non_image_features
if np.isnan(non_image_features).any() or np.isinf(non_image_features).any():
    print("Warning: NaN or inf values found in non_image_features. Replacing with the mean.")
    for col_idx in range(non_image_features.shape[1]):  # Iterate through columns
        col = non_image_features[:, col_idx]
        col_mean = np.nanmean(col)  # Calculate mean, ignoring NaNs
        col[np.isnan(col)] = col_mean  # Replace NaNs with the mean
    non_image_features = np.nan_to_num(non_image_features) # Replace any remaining NaN/inf with 0

print(f"Non-image features shape: {non_image_features.shape}")

# Extract basic image features
basic_image_features = np.array([
    np.concatenate([
        np.array([df['single_pixel_color_r'][i], df['single_pixel_color_g'][i], df['single_pixel_color_b'][i]]),
        np.array([df['weighted_average_color_r'][i], df['weighted_average_color_g'][i], df['weighted_average_color_b'][i]]),
        np.array([df['most_vibrant_color_r'][i], df['most_vibrant_color_g'][i], df['most_vibrant_color_b'][i]])
    ]) for i in range(len(df))
])

# Check for NaN or inf in basic_image_features
if np.isnan(basic_image_features).any() or np.isinf(basic_image_features).any():
    print("Warning: NaN or inf values found in basic_image_features. Replacing with the mean.")
    for col_idx in range(basic_image_features.shape[1]):  # Iterate through columns
        col = basic_image_features[:, col_idx]
        col_mean = np.nanmean(col)  # Calculate mean, ignoring NaNs
        col[np.isnan(col)] = col_mean  # Replace NaNs with the mean
    basic_image_features = np.nan_to_num(basic_image_features) # Replace any remaining NaN/inf with 0

print(f"Basic image features shape: {basic_image_features.shape}")

"""4. Feature Scaling"""

# Standardize the features
print("Standardizing features...")
scaler = StandardScaler()

non_image_features_scaled = scaler.fit_transform(non_image_features)
basic_image_features_scaled = scaler.fit_transform(basic_image_features)

# Standardize deep features if available
if deep_features is not None:
    deep_features_scaled = scaler.fit_transform(deep_features)
    print(f"Deep features scaled. Shape: {deep_features_scaled.shape}")
else:
    deep_features_scaled = None

"""5. Feature combinations"""

# Create different feature combinations
print("Creating feature combinations...")

# Model 1: Audio features only
features_audio = non_image_features_scaled
print(f"Audio features shape: {features_audio.shape}")

# Model 2: Basic image features only
features_basic_image = basic_image_features_scaled
print(f"Basic image features shape: {features_basic_image.shape}")

# Model 3: Audio + Basic image features
features_audio_basic_image = np.concatenate([non_image_features_scaled, basic_image_features_scaled], axis=1)
print(f"Audio + Basic image features shape: {features_audio_basic_image.shape}")

# Model 4: Deep learning features only (if available)
if deep_features_scaled is not None:
    features_deep = deep_features_scaled
    print(f"Deep features shape: {features_deep.shape}")

# Model 5: Audio + Deep features (if available)
if deep_features_scaled is not None:
    features_audio_deep = np.concatenate([non_image_features_scaled, deep_features_scaled], axis=1)
    print(f"Audio + Deep features shape: {features_audio_deep.shape}")

# Model 6: All features combined (if deep features available)
if deep_features_scaled is not None:
    features_all = np.concatenate([non_image_features_scaled, basic_image_features_scaled, deep_features_scaled], axis=1)
    print(f"All features combined shape: {features_all.shape}")

"""Feature Importance Analysis"""

# Feature Importance Analysis
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import pandas as pd
import seaborn as sns

print("Analyzing feature importance...")

# Define feature groups
audio_features = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness',
                 'instrumentalness', 'liveness', 'valence', 'tempo']

basic_image_features = ['single_pixel_color_r', 'single_pixel_color_g', 'single_pixel_color_b',
                       'weighted_average_color_r', 'weighted_average_color_g', 'weighted_average_color_b',
                       'most_vibrant_color_r', 'most_vibrant_color_g', 'most_vibrant_color_b']

# Create a target variable - we'll use a random forest to predict one feature from the others
# to understand feature relationships
target_feature = 'danceability'
features_to_use = [f for f in audio_features + basic_image_features if f != target_feature]

# Prepare data
X = df[features_to_use].copy()
y = df[target_feature].copy()

# Handle any missing values
X = X.fillna(X.mean())
y = y.fillna(y.mean())

# Train a Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)

# Get feature importances
feature_importances = rf_model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({
    'Feature': features_to_use,
    'Importance': feature_importances,
    'Category': ['Audio' if f in audio_features else 'Album Cover' for f in features_to_use]
})

# Sort by importance
importance_df = importance_df.sort_values('Importance', ascending=False)

# Create separate DataFrames for audio and album cover features
audio_importance = importance_df[importance_df['Category'] == 'Audio']
cover_importance = importance_df[importance_df['Category'] == 'Album Cover']

# Visualize feature importance
plt.figure(figsize=(14, 10))

# Plot 1: All features
plt.subplot(2, 1, 1)
sns.barplot(x='Importance', y='Feature', hue='Category', data=importance_df, palette=['#1DB954', '#FF6B6B'])
plt.title('Feature Importance (All Features)', fontsize=16)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.legend(title='Feature Type')
plt.grid(axis='x', linestyle='--', alpha=0.7)

# Plot 2: Split view - Audio vs Album Cover
plt.subplot(2, 2, 3)
sns.barplot(x='Importance', y='Feature', data=audio_importance, color='#1DB954')
plt.title('Audio Feature Importance', fontsize=14)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)

plt.subplot(2, 2, 4)
sns.barplot(x='Importance', y='Feature', data=cover_importance, color='#FF6B6B')
plt.title('Album Cover Feature Importance', fontsize=14)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

# Calculate and print summary statistics
print("\n===== FEATURE IMPORTANCE SUMMARY =====")
print(f"Total features analyzed: {len(features_to_use)}")
print(f"Audio features: {len(audio_features)-1} (excluding target feature)")
print(f"Album cover features: {len(basic_image_features)}")

# Calculate average importance by category
avg_audio_importance = audio_importance['Importance'].mean()
avg_cover_importance = cover_importance['Importance'].mean()
print(f"\nAverage importance - Audio features: {avg_audio_importance:.4f}")
print(f"Average importance - Album cover features: {avg_cover_importance:.4f}")

# Calculate total importance by category
total_audio_importance = audio_importance['Importance'].sum()
total_cover_importance = cover_importance['Importance'].sum()
total_importance = total_audio_importance + total_cover_importance
print(f"\nTotal contribution - Audio features: {total_audio_importance:.4f} ({total_audio_importance/total_importance:.2%})")
print(f"Total contribution - Album cover features: {total_cover_importance:.4f} ({total_cover_importance/total_importance:.2%})")

# Print top 3 most important features overall
print("\nTop 3 most important features overall:")
for i, row in importance_df.head(3).iterrows():
    print(f"  {row['Feature']} ({row['Category']}): {row['Importance']:.4f}")

# Print top 2 most important features by category
print("\nTop 2 most important audio features:")
for i, row in audio_importance.head(2).iterrows():
    print(f"  {row['Feature']}: {row['Importance']:.4f}")

print("\nTop 2 most important album cover features:")
for i, row in cover_importance.head(2).iterrows():
    print(f"  {row['Feature']}: {row['Importance']:.4f}")
print("=====================================")

"""Analyze Vector features"""

# Deep Learning Features Analysis
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE, MDS
import umap
import pandas as pd
import seaborn as sns
from scipy.stats import entropy
from sklearn.cluster import KMeans
import time

print("Analyzing deep learning features...")

# Check if deep features are available
if deep_features_scaled is None:
    print("Deep learning features are not available.")
else:
    # 1. Basic statistics of deep features
    feature_dim = deep_features_scaled.shape[1]
    print(f"Number of deep learning features: {feature_dim}")

    # Calculate variance of each feature
    feature_variance = np.var(deep_features_scaled, axis=0)

    # Plot feature variance distribution
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.hist(feature_variance, bins=50, color='purple', alpha=0.7)
    plt.title('Distribution of Feature Variance', fontsize=14)
    plt.xlabel('Variance', fontsize=12)
    plt.ylabel('Count', fontsize=12)
    plt.grid(alpha=0.3)

    # Plot cumulative variance explained
    sorted_variance = np.sort(feature_variance)[::-1]
    cumulative_variance = np.cumsum(sorted_variance) / np.sum(sorted_variance)

    plt.subplot(1, 2, 2)
    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'b-')
    plt.axhline(y=0.9, color='r', linestyle='--', alpha=0.5)
    plt.axhline(y=0.95, color='g', linestyle='--', alpha=0.5)
    plt.title('Cumulative Variance Explained', fontsize=14)
    plt.xlabel('Number of Features', fontsize=12)
    plt.ylabel('Cumulative Variance Ratio', fontsize=12)
    plt.grid(alpha=0.3)

    # Find number of features needed to explain 90% and 95% variance
    features_90 = np.where(cumulative_variance >= 0.9)[0][0] + 1
    features_95 = np.where(cumulative_variance >= 0.95)[0][0] + 1

    plt.annotate(f'90% variance: {features_90} features',
                 xy=(features_90, 0.9),
                 xytext=(features_90 + 20, 0.85),
                 arrowprops=dict(arrowstyle='->'))

    plt.annotate(f'95% variance: {features_95} features',
                 xy=(features_95, 0.95),
                 xytext=(features_95 + 20, 0.9),
                 arrowprops=dict(arrowstyle='->'))

    plt.tight_layout()
    plt.savefig('deep_features_variance.png', dpi=300, bbox_inches='tight')
    plt.show()

    print(f"\nFeature variance statistics:")
    print(f"  Min variance: {np.min(feature_variance):.6f}")
    print(f"  Max variance: {np.max(feature_variance):.6f}")
    print(f"  Mean variance: {np.mean(feature_variance):.6f}")
    print(f"  Median variance: {np.median(feature_variance):.6f}")
    print(f"  Number of features needed to explain 90% variance: {features_90}")
    print(f"  Number of features needed to explain 95% variance: {features_95}")

    # 2. Dimensionality Reduction for Visualization
    print("\nPerforming dimensionality reduction for visualization...")

    # Sample a subset of data if the dataset is large (for faster computation)
    sample_size = min(5000, deep_features_scaled.shape[0])
    indices = np.random.choice(deep_features_scaled.shape[0], sample_size, replace=False)
    sample_features = deep_features_scaled[indices]

    # Create a figure for dimensionality reduction visualizations
    plt.figure(figsize=(18, 12))

    # 2.1 PCA
    print("  Computing PCA...")
    start_time = time.time()
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(sample_features)
    pca_time = time.time() - start_time

    plt.subplot(2, 2, 1)
    plt.scatter(pca_result[:, 0], pca_result[:, 1], c='blue', alpha=0.5, s=20)
    plt.title(f'PCA (Explained Variance: {sum(pca.explained_variance_ratio_):.2%})\nTime: {pca_time:.2f}s', fontsize=14)
    plt.xlabel('Principal Component 1', fontsize=12)
    plt.ylabel('Principal Component 2', fontsize=12)
    plt.grid(alpha=0.3)

    # 2.2 t-SNE
    print("  Computing t-SNE...")
    start_time = time.time()
    tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)
    tsne_result = tsne.fit_transform(sample_features)
    tsne_time = time.time() - start_time

    plt.subplot(2, 2, 2)
    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c='green', alpha=0.5, s=20)
    plt.title(f't-SNE\nTime: {tsne_time:.2f}s', fontsize=14)
    plt.xlabel('t-SNE Component 1', fontsize=12)
    plt.ylabel('t-SNE Component 2', fontsize=12)
    plt.grid(alpha=0.3)

    # 2.3 UMAP
    print("  Computing UMAP...")
    start_time = time.time()
    reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)
    umap_result = reducer.fit_transform(sample_features)
    umap_time = time.time() - start_time

    plt.subplot(2, 2, 3)
    plt.scatter(umap_result[:, 0], umap_result[:, 1], c='red', alpha=0.5, s=20)
    plt.title(f'UMAP\nTime: {umap_time:.2f}s', fontsize=14)
    plt.xlabel('UMAP Component 1', fontsize=12)
    plt.ylabel('UMAP Component 2', fontsize=12)
    plt.grid(alpha=0.3)

    # 2.4 MDS (Multidimensional Scaling)
    print("  Computing MDS...")
    start_time = time.time()
    mds = MDS(n_components=2, random_state=42, n_jobs=-1)
    mds_result = mds.fit_transform(sample_features[:min(500, sample_size)])  # MDS is computationally expensive
    mds_time = time.time() - start_time

    plt.subplot(2, 2, 4)
    plt.scatter(mds_result[:, 0], mds_result[:, 1], c='purple', alpha=0.5, s=20)
    plt.title(f'MDS (on {min(500, sample_size)} samples)\nTime: {mds_time:.2f}s', fontsize=14)
    plt.xlabel('MDS Component 1', fontsize=12)
    plt.ylabel('MDS Component 2', fontsize=12)
    plt.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('deep_features_dimensionality_reduction.png', dpi=300, bbox_inches='tight')
    plt.show()

    # 3. Feature Importance via PCA
    print("\nAnalyzing feature importance via PCA...")
    pca_full = PCA(n_components=min(100, feature_dim))
    pca_full.fit(deep_features_scaled)

    # Plot explained variance ratio
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.bar(range(1, len(pca_full.explained_variance_ratio_) + 1),
            pca_full.explained_variance_ratio_, alpha=0.7, color='teal')
    plt.title('Explained Variance Ratio by Principal Component', fontsize=14)
    plt.xlabel('Principal Component', fontsize=12)
    plt.ylabel('Explained Variance Ratio', fontsize=12)
    plt.grid(alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),
             np.cumsum(pca_full.explained_variance_ratio_), 'b-')
    plt.title('Cumulative Explained Variance', fontsize=14)
    plt.xlabel('Number of Principal Components', fontsize=12)
    plt.ylabel('Cumulative Explained Variance', fontsize=12)
    plt.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('deep_features_pca_importance.png', dpi=300, bbox_inches='tight')
    plt.show()

    # 4. Cluster Analysis
    print("\nPerforming cluster analysis...")

    # Determine optimal number of clusters using the elbow method
    inertia = []
    k_range = range(2, 21)
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(sample_features)
        inertia.append(kmeans.inertia_)

    plt.figure(figsize=(10, 6))
    plt.plot(k_range, inertia, 'bo-')
    plt.title('Elbow Method for Optimal k', fontsize=14)
    plt.xlabel('Number of Clusters (k)', fontsize=12)
    plt.ylabel('Inertia', fontsize=12)
    plt.grid(alpha=0.3)
    plt.savefig('deep_features_elbow_method.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Find the elbow point
    from kneed import KneeLocator
    kneedle = KneeLocator(k_range, inertia, S=1.0, curve="convex", direction="decreasing")
    optimal_k = kneedle.elbow

    print(f"Optimal number of clusters (k): {optimal_k}")

    # Perform clustering with optimal k
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(sample_features)

    # Visualize clusters using PCA
    plt.figure(figsize=(12, 10))

    plt.subplot(2, 2, 1)
    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7, s=30)
    plt.title(f'PCA with {optimal_k} Clusters', fontsize=14)
    plt.xlabel('Principal Component 1', fontsize=12)
    plt.ylabel('Principal Component 2', fontsize=12)
    plt.colorbar(label='Cluster')
    plt.grid(alpha=0.3)

    # Visualize clusters using t-SNE
    plt.subplot(2, 2, 2)
    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7, s=30)
    plt.title(f't-SNE with {optimal_k} Clusters', fontsize=14)
    plt.xlabel('t-SNE Component 1', fontsize=12)
    plt.ylabel('t-SNE Component 2', fontsize=12)
    plt.colorbar(label='Cluster')
    plt.grid(alpha=0.3)

    # Visualize clusters using UMAP
    plt.subplot(2, 2, 3)
    plt.scatter(umap_result[:, 0], umap_result[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7, s=30)
    plt.title(f'UMAP with {optimal_k} Clusters', fontsize=14)
    plt.xlabel('UMAP Component 1', fontsize=12)
    plt.ylabel('UMAP Component 2', fontsize=12)
    plt.colorbar(label='Cluster')
    plt.grid(alpha=0.3)

    # Cluster sizes
    plt.subplot(2, 2, 4)
    cluster_sizes = np.bincount(cluster_labels)
    plt.bar(range(len(cluster_sizes)), cluster_sizes, color='teal', alpha=0.7)
    plt.title('Cluster Sizes', fontsize=14)
    plt.xlabel('Cluster', fontsize=12)
    plt.ylabel('Number of Samples', fontsize=12)
    plt.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig('deep_features_clustering.png', dpi=300, bbox_inches='tight')
    plt.show()

    # 5. Feature Correlation Analysis
    print("\nAnalyzing feature correlations...")

    # Compute correlation between deep features and audio features
    # Sample a subset of features for correlation analysis
    num_deep_features = min(50, deep_features_scaled.shape[1])
    selected_deep_features = deep_features_scaled[:, :num_deep_features]

    # Create a correlation matrix between audio features and selected deep features
    audio_data = df[audio_features].values

    # Ensure both matrices have the same number of samples
    assert audio_data.shape[0] == selected_deep_features.shape[0], "Sample count mismatch"

    # Calculate correlation matrix
    correlation_matrix = np.zeros((len(audio_features), num_deep_features))
    for i, audio_feat in enumerate(audio_features):
        for j in range(num_deep_features):
            correlation_matrix[i, j] = np.corrcoef(audio_data[:, i], selected_deep_features[:, j])[0, 1]

    # Plot correlation heatmap
    plt.figure(figsize=(14, 8))
    sns.heatmap(correlation_matrix, cmap='coolwarm', center=0,
                xticklabels=[f'Deep_{i+1}' for i in range(num_deep_features)],
                yticklabels=audio_features)
    plt.title('Correlation between Audio Features and Deep Features', fontsize=14)
    plt.xlabel('Deep Features', fontsize=12)
    plt.ylabel('Audio Features', fontsize=12)
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig('deep_audio_correlation.png', dpi=300, bbox_inches='tight')
    plt.show()

    # 6. Summary Statistics
    print("\n===== DEEP LEARNING FEATURES SUMMARY =====")
    print(f"Total number of deep features: {feature_dim}")
    print(f"Features needed for 90% variance: {features_90} ({features_90/feature_dim:.2%} of total)")
    print(f"Features needed for 95% variance: {features_95} ({features_95/feature_dim:.2%} of total)")
    print(f"Optimal number of clusters: {optimal_k}")

    # Find most correlated audio features with deep features
    max_correlations = np.max(np.abs(correlation_matrix), axis=1)
    audio_deep_correlation = pd.DataFrame({
        'Audio Feature': audio_features,
        'Max Correlation with Deep Features': max_correlations
    }).sort_values('Max Correlation with Deep Features', ascending=False)

    print("\nAudio features most correlated with deep features:")
    for i, row in audio_deep_correlation.iterrows():
        print(f"  {row['Audio Feature']}: {row['Max Correlation with Deep Features']:.4f}")

    print("==========================================")

"""6. Voyager Index Building"""

# Build Voyager indices for each feature set and save to Google Drive
import os
from voyager import Index, Space

# First, ensure we have the data folder path defined
data_folder = '/content/drive/MyDrive/complete_data_set_27_02_25'
# Create the folder if it doesn't exist
os.makedirs(data_folder, exist_ok=True)

print("Building Voyager indices and saving to Google Drive...")
# Parameters for Voyager index
M = 12  # Number of connections between nodes (similar to Annoy's n_trees)
ef_construction = 200  # Number of vectors to search during construction

# Model 1: Audio features
index_audio = Index(Space.Euclidean, num_dimensions=features_audio.shape[1],
                    M=M, ef_construction=ef_construction)
# Add items to the index
for i in range(len(features_audio)):
    index_audio.add_item(features_audio[i].astype(np.float32), i)
audio_path = os.path.join(data_folder, 'audio_features.voy')
index_audio.save(audio_path)
print(f"Audio features index built and saved to: {audio_path}")

# Model 2: Basic image features
index_basic_image = Index(Space.Euclidean, num_dimensions=features_basic_image.shape[1],
                          M=M, ef_construction=ef_construction)
for i in range(len(features_basic_image)):
    index_basic_image.add_item(features_basic_image[i].astype(np.float32), i)
basic_image_path = os.path.join(data_folder, 'basic_image_features.voy')
index_basic_image.save(basic_image_path)
print(f"Basic image features index built and saved to: {basic_image_path}")

# Model 3: Audio + Basic image features
index_audio_basic_image = Index(Space.Euclidean, num_dimensions=features_audio_basic_image.shape[1],
                                M=M, ef_construction=ef_construction)
for i in range(len(features_audio_basic_image)):
    index_audio_basic_image.add_item(features_audio_basic_image[i].astype(np.float32), i)
audio_basic_image_path = os.path.join(data_folder, 'audio_basic_image_features.voy')
index_audio_basic_image.save(audio_basic_image_path)
print(f"Audio + Basic image features index built and saved to: {audio_basic_image_path}")

# Model 4: Deep learning features (if available)
if deep_features_scaled is not None:
    index_deep = Index(Space.Euclidean, num_dimensions=features_deep.shape[1],
                       M=M, ef_construction=ef_construction)
    for i in range(len(features_deep)):
        index_deep.add_item(features_deep[i].astype(np.float32), i)
    deep_path = os.path.join(data_folder, 'deep_features.voy')
    index_deep.save(deep_path)
    print(f"Deep features index built and saved to: {deep_path}")

# Model 5: All Image features
if deep_features_scaled is not None:
    # Combine basic image features with deep features
    features_all_image = np.hstack([features_basic_image, deep_features_scaled])

    index_all_image = Index(Space.Euclidean, num_dimensions=features_all_image.shape[1],
                           M=M, ef_construction=ef_construction)
    for i in range(len(features_all_image)):
        index_all_image.add_item(features_all_image[i].astype(np.float32), i)
    all_image_path = os.path.join(data_folder, 'all_image_features.voy')
    index_all_image.save(all_image_path)
    print(f"All image features combined index built and saved to: {all_image_path}")
else:
    print("Deep features not available, skipping all image features model")

# Model 6: Audio + Deep features (if available)
if deep_features_scaled is not None:
    index_audio_deep = Index(Space.Euclidean, num_dimensions=features_audio_deep.shape[1],
                             M=M, ef_construction=ef_construction)
    for i in range(len(features_audio_deep)):
        index_audio_deep.add_item(features_audio_deep[i].astype(np.float32), i)
    audio_deep_path = os.path.join(data_folder, 'audio_deep_features.voy')
    index_audio_deep.save(audio_deep_path)
    print(f"Audio + Deep features index built and saved to: {audio_deep_path}")

# Model 7: All features combined (if deep features available)
if deep_features_scaled is not None:
    index_all = Index(Space.Euclidean, num_dimensions=features_all.shape[1],
                      M=M, ef_construction=ef_construction)
    for i in range(len(features_all)):
        index_all.add_item(features_all[i].astype(np.float32), i)
    all_features_path = os.path.join(data_folder, 'all_features.voy')
    index_all.save(all_features_path)
    print(f"All features combined index built and saved to: {all_features_path}")

print("\nAll Voyager indices have been built and saved to Google Drive.")

"""7. Recommendation Functions"""

# Function to get song recommendations
def get_recommendations(song_id, num_recommendations=10):
    """
    Get song recommendations based on different feature sets

    Args:
        song_id (str): Spotify ID of the query song
        num_recommendations (int): Number of recommendations to return

    Returns:
        dict: Dictionary with recommendations from different models
    """
    # Find the song in the DataFrame
    if song_id not in df['id'].values:
        print(f"Song ID {song_id} not found in the dataset.")
        return None

    song_index = df[df['id'] == song_id].index[0]
    song_data = df.iloc[song_index]

    print("\n===== QUERY SONG =====")
    print(f"Song: {song_data['name']} by {song_data['artist']}")
    print(f"Spotify ID: {song_data['id']}")
    print(f"Image URL: {song_data['img']}")
    print(f"Preview URL: {song_data['preview']}")
    print("======================\n")

    recommendations = {}

    # Model 1: Audio features
    index_audio = Index.load(os.path.join(data_folder, 'audio_features.voy'))
    # Get nearest neighbors
    closest_indices_audio, distances_audio = index_audio.query(
        features_audio[song_index].astype(np.float32), k=num_recommendations+1)
    # Remove the query song itself (first result)
    mask_audio = closest_indices_audio != song_index
    closest_indices_audio = closest_indices_audio[mask_audio][:num_recommendations]
    recommendations['audio'] = df.iloc[closest_indices_audio]

    # Model 2: Basic image features
    index_basic_image = Index.load(os.path.join(data_folder, 'basic_image_features.voy'))
    closest_indices_basic_image, distances_basic_image = index_basic_image.query(
        features_basic_image[song_index].astype(np.float32), k=num_recommendations+1)
    mask_basic_image = closest_indices_basic_image != song_index
    closest_indices_basic_image = closest_indices_basic_image[mask_basic_image][:num_recommendations]
    recommendations['basic_image'] = df.iloc[closest_indices_basic_image]

    # Model 3: Audio + Basic image features
    index_audio_basic_image = Index.load(os.path.join(data_folder, 'audio_basic_image_features.voy'))
    closest_indices_audio_basic_image, distances_audio_basic_image = index_audio_basic_image.query(
        features_audio_basic_image[song_index].astype(np.float32), k=num_recommendations+1)
    mask_audio_basic_image = closest_indices_audio_basic_image != song_index
    closest_indices_audio_basic_image = closest_indices_audio_basic_image[mask_audio_basic_image][:num_recommendations]
    recommendations['audio_basic_image'] = df.iloc[closest_indices_audio_basic_image]

    # Model 4: Deep learning features (if available)
    if deep_features_scaled is not None:
        index_deep = Index.load(os.path.join(data_folder, 'deep_features.voy'))
        closest_indices_deep, distances_deep = index_deep.query(
            features_deep[song_index].astype(np.float32), k=num_recommendations+1)
        mask_deep = closest_indices_deep != song_index
        closest_indices_deep = closest_indices_deep[mask_deep][:num_recommendations]
        recommendations['deep'] = df.iloc[closest_indices_deep]

    # Model 5: All IMage Features
    if deep_features_scaled is not None:
        # Load the all image features index
        index_all_image = Index.load(os.path.join(data_folder, 'all_image_features.voy'))
        # Create the combined feature vector for the query song
        features_all_image_query = np.hstack([
            features_basic_image[song_index],
            deep_features_scaled[song_index]
        ]).astype(np.float32)
        # Get recommendations
        closest_indices_all_image, distances_all_image = index_all_image.query(
            features_all_image_query, k=num_recommendations+1)
        mask_all_image = closest_indices_all_image != song_index
        closest_indices_all_image = closest_indices_all_image[mask_all_image][:num_recommendations]
        recommendations['all_image'] = df.iloc[closest_indices_all_image]

    # Model 6: Audio + Deep features (if available)
    if deep_features_scaled is not None:
        index_audio_deep = Index.load(os.path.join(data_folder, 'audio_deep_features.voy'))
        closest_indices_audio_deep, distances_audio_deep = index_audio_deep.query(
            features_audio_deep[song_index].astype(np.float32), k=num_recommendations+1)
        mask_audio_deep = closest_indices_audio_deep != song_index
        closest_indices_audio_deep = closest_indices_audio_deep[mask_audio_deep][:num_recommendations]
        recommendations['audio_deep'] = df.iloc[closest_indices_audio_deep]

    # Model 7: All features combined (if deep features available)
    if deep_features_scaled is not None:
        index_all = Index.load(os.path.join(data_folder, 'all_features.voy'))
        closest_indices_all, distances_all = index_all.query(
            features_all[song_index].astype(np.float32), k=num_recommendations+1)
        mask_all = closest_indices_all != song_index
        closest_indices_all = closest_indices_all[mask_all][:num_recommendations]
        recommendations['all'] = df.iloc[closest_indices_all]

    return recommendations

# Function to print recommendations
def print_recommendations(recommendations):
    """Print recommendations from different models"""
    if recommendations is None:
        return

    for model_name, recs in recommendations.items():
        print(f"\n===== RECOMMENDATIONS USING {model_name.upper()} FEATURES =====")
        for i, (_, song) in enumerate(recs.iterrows()):
            print(f"{i+1}. {song['name']} by {song['artist']}")
            print(f"   Spotify ID: {song['id']}")
            print(f"   Image URL: {song['img']}")
            print(f"   Preview URL: {song['preview']}")
            print("   ---")
        print(f"===== END OF {model_name.upper()} RECOMMENDATIONS =====\n")

"""8. Example"""

# Example usage
if __name__ == "__main__":
    # You can replace this with any song ID from your dataset
    query_song_id = '0dqGfCMAGyDgpUAgLNOjWd'

    # Get recommendations
    recommendations = get_recommendations(query_song_id, num_recommendations=5)

    # Print recommendations
    print_recommendations(recommendations)

# Example usage
if __name__ == "__main__":
    # You can replace this with any song ID from your dataset
    query_song_id = '2WfaOiMkCvy7F5fcp2zZ8L'

    # Get recommendations
    recommendations = get_recommendations(query_song_id, num_recommendations=5)

    # Print recommendations
    print_recommendations(recommendations)

# Example usage
if __name__ == "__main__":
    # You can replace this with any song ID from your dataset
    query_song_id = '4vUmTMuQqjdnvlZmAH61Qk'

    # Get recommendations
    recommendations = get_recommendations(query_song_id, num_recommendations=5)

    # Print recommendations
    print_recommendations(recommendations)

"""9. Setup for visualization"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from voyager import Index, Space
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import random
import os
import pickle

# Choose which features to visualize
feature_type = 'basic_image'  # Options: 'audio', 'basic_image', 'audio_basic_image', 'deep', 'audio_deep', 'all'

if feature_type == 'audio':
    features = non_image_features_scaled
    title = 'Spotify Songs - Audio Features'
    voyager_index_file = 'audio_features.voy'
elif feature_type == 'basic_image':
    features = basic_image_features_scaled
    title = 'Spotify Songs - Cover Art Features'
    voyager_index_file = 'basic_image_features.voy'
elif feature_type == 'audio_basic_image':
    features = np.concatenate([non_image_features_scaled, basic_image_features_scaled], axis=1)
    title = 'Spotify Songs - Combined Audio and Visual Features'
    voyager_index_file = 'audio_basic_image_features.voy'
elif feature_type == 'deep' and deep_features_scaled is not None:
    features = deep_features_scaled
    title = 'Spotify Songs - Deep Learning Features'
    voyager_index_file = 'deep_features.voy'
elif feature_type == 'audio_deep' and deep_features_scaled is not None:
    features = np.concatenate([non_image_features_scaled, deep_features_scaled], axis=1)
    title = 'Spotify Songs - Combined Audio and Deep Learning Features'
    voyager_index_file = 'audio_deep_features.voy'
elif feature_type == 'all' and deep_features_scaled is not None:
    features = np.concatenate([non_image_features_scaled, basic_image_features_scaled, deep_features_scaled], axis=1)
    title = 'Spotify Songs - All Features Combined'
    voyager_index_file = 'all_features.voy'
else:
    print("Invalid feature type or deep features not available. Falling back to audio features.")
    features = non_image_features_scaled
    title = 'Spotify Songs - Audio Features'
    voyager_index_file = 'audio_features.voy'

# Reduce to 2D using PCA for visualization
pca = PCA(n_components=2)
features_2d = pca.fit_transform(features)
print(f"Explained variance ratio: {pca.explained_variance_ratio_}")

# Load Voyager Index
voyager_index_path = os.path.join(data_folder, voyager_index_file)
if os.path.exists(voyager_index_path):
    index = Index.load(voyager_index_path)
    print(f"Loaded Voyager index from {voyager_index_path}")
else:
    print(f"Voyager index file not found at {voyager_index_path}. Creating new index...")
    index = Index(Space.Euclidean, num_dimensions=features.shape[1],
                  M=12, ef_construction=200)

    # Add items to the index
    for i in range(len(features)):
        index.add_item(features[i].astype(np.float32), i)

    # Save the index
    index.save(voyager_index_path)
    print(f"Created and saved new Voyager index to {voyager_index_path}")

"""10. 2D Viz"""

"""Cell 10: 2D Visualization (Matplotlib)"""

# Function to assign colors to clusters using Voyager
def get_clusters_voyager(index, features, n_clusters=20):
    """
    Assigns data points to clusters using Voyager's nearest neighbor search.

    Args:
        index (Index): The Voyager index.
        features (np.array): The feature data.
        n_clusters (int): The number of clusters to create.

    Returns:
        dict: A dictionary where keys are cluster IDs and values are lists of data point indices.
    """
    clusters = {}
    # Choose cluster centers randomly
    all_indices = list(range(len(features)))
    random.shuffle(all_indices)
    cluster_centers = all_indices[:n_clusters]

    # Initialize clusters with their center points
    for i, center_index in enumerate(cluster_centers):
        clusters[i] = [center_index]

    # For each data point, find the nearest cluster center
    for i in range(len(features)):
        if i not in cluster_centers:  # Skip cluster centers
            min_distance = float('inf')
            nearest_cluster = None

            for cluster_id, center_index in enumerate(cluster_centers):
                # Calculate distance between point and cluster center
                distance = index.get_distance(
                    features[i].astype(np.float32),
                    features[center_index].astype(np.float32)
                )

                if distance < min_distance:
                    min_distance = distance
                    nearest_cluster = cluster_id

            if nearest_cluster is not None:
                clusters[nearest_cluster].append(i)

    return clusters

# Get clusters using Voyager
clusters = get_clusters_voyager(index, features, n_clusters=20)

# Create a colormap
colors = plt.cm.tab20(np.linspace(0, 1, len(clusters)))
additional_colors = plt.cm.Set3(np.linspace(0, 1, max(0, len(clusters) - 20)))
if len(clusters) > 20:
    colors = np.vstack([colors, additional_colors])

# Prepare the plot
plt.figure(figsize=(10, 10))

# Plot each cluster with its own color
for i, (cluster_id, points) in enumerate(clusters.items()):
    x = [features_2d[p, 0] for p in points]
    y = [features_2d[p, 1] for p in points]
    plt.scatter(x, y, c=[colors[i]], marker='x', alpha=0.7, s=30)

# Add titles and labels
plt.title(title, fontsize=15)
plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')

# Optional: Add a legend showing some of the clusters
legend_entries = min(10, len(clusters))  # Comment this out if you have too many clusters
handles = [plt.Line2D([0], [0], marker='x', color='w', markerfacecolor=colors[i],
                      markersize=10, label=f'Cluster {i + 1}')
           for i in range(legend_entries)]
plt.legend(handles=handles, loc='upper right')

# Optional: Annotate some points with song names
for i, (_, points) in enumerate(list(clusters.items())[:5]):  # Just label a few clusters
    for point in points[:3]:  # Just label a few points per cluster
        plt.annotate(df['name'].iloc[point][:15],
                     (features_2d[point, 0], features_2d[point, 1]),
                     fontsize=8)
plt.tight_layout()
plt.savefig('spotify_visualization.png', dpi=300)
plt.show()

"""Cell 11: 3D Visualization"""

# 3D Visualization (Plotly) with Corrected JavaScript Order
import plotly.express as px
from sklearn.decomposition import PCA
import pandas as pd
import json
import numpy as np

# Reduce to 3D
pca_3d = PCA(n_components=3)
features_3d = pca_3d.fit_transform(features)

# Create a DataFrame for plotly
plot_df = pd.DataFrame({
    'PC1': features_3d[:, 0],
    'PC2': features_3d[:, 1],
    'PC3': features_3d[:, 2],
    'Song': df['name'],
    'Artist': df['artist'],
    'Index': df.index  # Add the index to the DataFrame
})

# Assign cluster colors
cluster_labels = np.zeros(len(df), dtype=int)
for cluster_id, points in clusters.items():
    for point in points:
        cluster_labels[point] = cluster_id

plot_df['Cluster'] = cluster_labels

# Create the 3D scatter plot
fig = px.scatter_3d(
    plot_df, x='PC1', y='PC2', z='PC3',
    color='Cluster', hover_data=['Song', 'Artist'],
    title='3D Visualization of Spotify Songs'
)

fig.update_layout(scene=dict(
    xaxis_title=f'PC1 ({pca_3d.explained_variance_ratio_[0]:.2%})',
    yaxis_title=f'PC2 ({pca_3d.explained_variance_ratio_[1]:.2%})',
    zaxis_title=f'PC3 ({pca_3d.explained_variance_ratio_[2]:.2%})'
))

# Add reset button
fig.update_layout(
    title='3D Visualization of Spotify Songs',
    updatemenus=[
        dict(
            type="buttons",
            showactive=False,
            buttons=[
                dict(
                    label="Reset",
                    method="relayout",
                    args=["scene.camera", None]
                )
            ]
        )
    ]
)

# Add annotations for search
fig.update_layout(
    margin=dict(l=0, r=0, b=0, t=80),
    height=800,
    width=1200
)

# Create a full HTML document with proper script loading order
full_html = """
<!DOCTYPE html>
<html>
<head>
    <title>Spotify 3D Visualization</title>
    <script src="https://cdn.plot.ly/plotly-2.27.1.min.js"></script>
    <style>
    .search-container {
        position: relative;
        width: 400px;
        margin: 20px auto;
        display: flex;
        flex-direction: column;
    }

    .search-input-group {
        display: flex;
        width: 100%;
    }

    .search-input {
        flex-grow: 1;
        padding: 10px;
        border: 1px solid #ccc;
        border-radius: 5px 0 0 5px;
        font-size: 16px;
    }

    .search-button {
        padding: 10px 15px;
        background-color: #4CAF50;
        color: white;
        border: none;
        border-radius: 0 5px 5px 0;
        cursor: pointer;
    }

    .autocomplete-list {
        position: absolute;
        top: 40px;
        left: 0;
        width: 100%;
        background-color: white;
        border: 1px solid #ccc;
        border-radius: 0 0 5px 5px;
        z-index: 1000;
        display: none;
        max-height: 200px;
        overflow-y: auto;
    }

    .autocomplete-item {
        padding: 10px;
        cursor: pointer;
    }

    .autocomplete-item:hover {
        background-color: #f0f0f0;
    }

    .search-feedback {
        color: red;
        margin-top: 5px;
    }

    h1 {
        text-align: center;
        margin-bottom: 10px;
    }
    </style>
</head>
<body>
    <h1>3D Visualization of Spotify Songs</h1>

    <div class="search-container">
        <div class="search-input-group">
            <input type="text" id="search-input" class="search-input" placeholder="Search for a song...">
            <button id="search-button" class="search-button">Search</button>
        </div>
        <div id="search-feedback" class="search-feedback"></div>
        <div id="autocomplete-list" class="autocomplete-list"></div>
    </div>
"""

# Add the Plotly figure
full_html += fig.to_html(include_plotlyjs=False, full_html=False)

# Add the JavaScript for search functionality
songs_json = json.dumps(plot_df[['Song', 'Artist', 'Index']].to_dict(orient='records'), ensure_ascii=False)

full_html += f"""
    <script>
        // Store all songs data
        const allSongs = {songs_json};
        let selectedIndex = null;

        // Wait for the Plotly graph to be fully loaded and initialized
        window.addEventListener('load', function() {{
            console.log('Window loaded');

            // Make sure the graph div exists and is initialized
            setTimeout(function() {{
                const graphDiv = document.getElementById('plotly-graph-div');
                console.log('Graph div:', graphDiv);

                if (!graphDiv || !graphDiv.data) {{
                    console.error('Plotly graph not initialized yet');
                    return;
                }}

                function highlightPoint(index) {{
                    console.log('Highlighting point:', index);

                    try {{
                        if (index === null) {{
                            // Reset all points to default size
                            Plotly.restyle(graphDiv, {{'marker.size': 8}}, [0]);
                            return;
                        }}

                        // Create an array of sizes where only the selected point is larger
                        const sizes = Array(allSongs.length).fill(8);
                        sizes[index] = 16;

                        // Update the marker sizes
                        Plotly.restyle(graphDiv, {{'marker.size': [sizes]}}, [0]);

                        // Center the camera on the selected point
                        const point = {{
                            x: graphDiv.data[0].x[index],
                            y: graphDiv.data[0].y[index],
                            z: graphDiv.data[0].z[index]
                        }};

                        // Animate to the point
                        const camera = {{
                            eye: {{
                                x: point.x + 1.5,
                                y: point.y + 1.5,
                                z: point.z + 1.5
                            }},
                            center: {{
                                x: point.x,
                                y: point.y,
                                z: point.z
                            }}
                        }};

                        Plotly.relayout(graphDiv, {{'scene.camera': camera}});
                    }} catch (error) {{
                        console.error('Error highlighting point:', error);
                    }}
                }}

                function autocomplete(query) {{
                    const autocompleteList = document.getElementById('autocomplete-list');
                    const searchFeedback = document.getElementById('search-feedback');
                    autocompleteList.innerHTML = '';
                    autocompleteList.style.display = 'none';
                    searchFeedback.textContent = "Searching...";

                    if (!query) {{
                        searchFeedback.textContent = "";
                        return;
                    }}

                    const trimmedQuery = query.trim().toLowerCase();

                    const matches = allSongs.filter(song => {{
                        const songName = (song.Song || '').trim().toLowerCase();
                        const artistName = (song.Artist || '').trim().toLowerCase();
                        return songName.includes(trimmedQuery) || artistName.includes(trimmedQuery);
                    }});

                    searchFeedback.textContent = "";

                    if (matches.length > 0) {{
                        autocompleteList.style.display = 'block';
                        matches.slice(0, 10).forEach(match => {{
                            const item = document.createElement('div');
                            item.classList.add('autocomplete-item');
                            item.textContent = match.Song + ' - ' + match.Artist;
                            item.addEventListener('click', function() {{
                                document.getElementById('search-input').value = match.Song + ' - ' + match.Artist;
                                highlightPoint(match.Index);
                                autocompleteList.style.display = 'none';
                                selectedIndex = match.Index;
                            }});
                            autocompleteList.appendChild(item);
                        }});

                        if (matches.length > 10) {{
                            const item = document.createElement('div');
                            item.classList.add('autocomplete-item');
                            item.textContent = `... and ${{matches.length - 10}} more matches`;
                            autocompleteList.appendChild(item);
                        }}
                    }} else {{
                        searchFeedback.textContent = "No matches found.";
                    }}
                }}

                document.getElementById('search-input').addEventListener('input', function(e) {{
                    autocomplete(e.target.value);
                }});

                document.getElementById('search-button').addEventListener('click', function() {{
                    const query = document.getElementById('search-input').value;
                    autocomplete(query);
                }});

                // Close autocomplete when clicking outside
                document.addEventListener('click', function(e) {{
                    if (!e.target.closest('.search-container')) {{
                        document.getElementById('autocomplete-list').style.display = 'none';
                    }}
                }});

                // Initialize with all points at default size
                highlightPoint(null);

                console.log('Search functionality initialized');
            }}, 1000); // Wait 1 second to ensure Plotly is fully initialized
        }});
    </script>
</body>
</html>
"""

# Save the combined HTML to a file
with open('spotify_visualization_3d.html', 'w', encoding='utf-8') as f:
    f.write(full_html)

# Display the plot in the notebook
fig.show()